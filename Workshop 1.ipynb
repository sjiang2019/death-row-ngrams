{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1\n",
    "#### Steven Jiang\n",
    "#### October 11, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Last Statements Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "statement_text = []\n",
    "\n",
    "death_row = requests.get(\"http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html\")\n",
    "soup = BeautifulSoup(death_row.text,\"html.parser\")\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    link = cols[2].find('a').get('href')\n",
    "    if \"no_last_statement\" not in link:\n",
    "        try:\n",
    "            link = \"http://www.tdcj.state.tx.us/death_row/\" + link\n",
    "            statement_page = requests.get(link)\n",
    "            statement_soup = BeautifulSoup(statement_page.text,\"html.parser\")\n",
    "            text = statement_soup.find_all('p')[-1].text\n",
    "            if text != \"No statement.\" and len(text) > 0:\n",
    "                statement_text += sent_tokenize(text)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Hard West Turn Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hard_west_turn.txt\", 'r') as f:\n",
    "    hwt_text = sent_tokenize(f.read().decode(\"utf-8\").strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('last_statements_sents.pkl', 'wb') as ls:\n",
    "    pickle.dump(statement_text, ls)\n",
    "with open('hwt_text.pkl', 'wb') as hwt:\n",
    "    pickle.dump(hwt_text, hwt)\n",
    "with open('all_text.pkl', 'wb') as all_txt_file:\n",
    "    all_txt = statement_text+hwt_text\n",
    "    pickle.dump(all_txt, all_txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('last_statements_sents.pkl', 'rb') as ls:\n",
    "    statement_text = pickle.load(ls)\n",
    "with open('hwt_text.pkl', 'rb') as hwt:\n",
    "    hwt_text = pickle.load(hwt)\n",
    "with open('all_text.pkl', 'rb') as all_txt_file:\n",
    "    all_text = pickle.load(all_txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Get counts of individual words\n",
    "def gen_unigram_counts(unigram_count, word_list, i):\n",
    "    if word_list[i] in unigram_count.keys():\n",
    "        unigram_count[word_list[i]] += 1\n",
    "    else:\n",
    "        unigram_count[word_list[i]] = 1\n",
    "\n",
    "# Get counts of bigrams\n",
    "def gen_bigram_counts(bigram_count, word_list, i):\n",
    "    # Edge case for first word\n",
    "    if i == 0:\n",
    "        bigram_key = (word_list[i], \"#\")\n",
    "    else:\n",
    "        bigram_key = (word_list[i], word_list[i-1])\n",
    "    if bigram_key in bigram_count.keys():\n",
    "        bigram_count[bigram_key] += 1\n",
    "    else:\n",
    "        bigram_count[bigram_key] = 1\n",
    "\n",
    "# Get counts of trigrams\n",
    "def gen_trigram_counts(trigram_count, word_list, i):\n",
    "    # Edge cases for first and second word\n",
    "    if i == 0:\n",
    "        trigram_key = (word_list[i], \"#\", \"#\")\n",
    "    elif i == 1:\n",
    "        trigram_key = (word_list[i], word_list[i-1], \"#\")\n",
    "    else:\n",
    "        trigram_key = (word_list[i], word_list[i-1], word_list[i-2])\n",
    "    if trigram_key in trigram_count.keys():\n",
    "        trigram_count[trigram_key] += 1\n",
    "    else:\n",
    "        trigram_count[trigram_key] = 1\n",
    "\n",
    "# Generate bigram and trigram probability dictionaries\n",
    "def gen_probs(raw_text):\n",
    "    # Keep count for each word\n",
    "    unigram_count = {}\n",
    "    bigram_count = {}\n",
    "    trigram_count = {}\n",
    "    # Initialize the count for the start keys to be 0\n",
    "    unigram_count[\"#\"] = 0\n",
    "    bigram_count[(\"#\", \"#\")] = 0\n",
    "    # Iterate over lines in file\n",
    "    for sent in raw_text:\n",
    "        # Get list of words\n",
    "        word_list = sent.split()\n",
    "        # Increment the count for the word\n",
    "        unigram_count[\"#\"] += 1\n",
    "        bigram_count[(\"#\", \"#\")] += 1\n",
    "        for i in range(len(word_list)):\n",
    "            gen_unigram_counts(unigram_count, word_list, i)\n",
    "            gen_bigram_counts(bigram_count, word_list, i)\n",
    "            gen_trigram_counts(trigram_count, word_list, i)\n",
    "        # Add stop symbol into dictionary\n",
    "        last_bigram_key = (\"#\",word_list[-1])\n",
    "        if last_bigram_key in bigram_count.keys():\n",
    "            bigram_count[last_bigram_key] += 1\n",
    "        else:\n",
    "            bigram_count[last_bigram_key] = 1\n",
    "        if len(word_list) >1:\n",
    "            last_trigram_key = (\"#\",word_list[-1], word_list[-2])\n",
    "        else:\n",
    "            last_trigram_key = (\"#\",word_list[-1], \"#\")\n",
    "        if last_trigram_key in trigram_count.keys():\n",
    "            trigram_count[last_trigram_key] += 1\n",
    "        else:\n",
    "            trigram_count[last_trigram_key] = 1\n",
    "    # Calculate probabilities\n",
    "    trigram_prob = {}\n",
    "    for key in trigram_count.iterkeys():\n",
    "        word1 = key[1]\n",
    "        word2 = key[2]\n",
    "        trigram_prob[key] = trigram_count[key]/float(bigram_count[(word1, word2)])\n",
    "    return trigram_prob\n",
    "\n",
    "# Generate a word with the trigram model\n",
    "def gen_sent_trigram(trigram_prob):\n",
    "    # Begin with the start symbol\n",
    "    sent = \"#\"\n",
    "    # Keep track of last two\n",
    "    start_key = \"\"\n",
    "    prev_key = \"\"\n",
    "    # Randomly choose words until we reach a stop symbol\n",
    "    num_words = 0\n",
    "    while start_key != \"#\":\n",
    "        num_words += 1\n",
    "        if num_words > 20:\n",
    "            return None\n",
    "        if start_key == \"\" and prev_key == \"\":\n",
    "            start_key = \"#\"\n",
    "            prev_key = \"#\"\n",
    "        keys = []\n",
    "        for key in trigram_prob.iterkeys():\n",
    "            if (key[1], key[2]) == (start_key, prev_key):\n",
    "                keys.append(key)\n",
    "        choices = []\n",
    "        probs = []\n",
    "        for c in keys:\n",
    "            choices.append(c[0])\n",
    "            probs.append(trigram_prob[c])\n",
    "        prev_key = start_key\n",
    "        start_key = np.random.choice(choices, p=probs)\n",
    "        sent = sent + \" \" + start_key\n",
    "\n",
    "    return sent[2:-1]\n",
    "\n",
    "# Function for generating sentences\n",
    "def gen_sents(prob, n):\n",
    "    sentences = []\n",
    "    for i in range(n):\n",
    "        sent = gen_sent_trigram(prob)\n",
    "        if sent:\n",
    "            sentences.append(sent)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ls_trigram = gen_probs(statement_text)\n",
    "with open('ls_trigram.pkl', 'wb') as ls:\n",
    "    pickle.dump(ls_trigram, ls)\n",
    "\n",
    "hwt_trigram = gen_probs(hwt_text)\n",
    "with open('hwt_trigram.pkl', 'wb') as hwt:\n",
    "    pickle.dump(hwt_trigram, hwt)\n",
    "\n",
    "all_trigram = gen_probs(all_text)\n",
    "with open('all_trigram.pkl', 'wb') as all_tri:\n",
    "    pickle.dump(all_trigram, all_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ls_trigram.pkl', 'rb') as ls:\n",
    "    ls_trigram = pickle.load(ls)\n",
    "with open('hwt_trigram.pkl', 'rb') as hwt:\n",
    "    hwt_trigram = pickle.load(hwt)\n",
    "with open('all_trigram.pkl', 'rb') as all_tri:\n",
    "    all_trigram = pickle.load(all_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sentences Using Trigram Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_sentences = gen_sents(ls_trigram, 15)\n",
    "hwt_sentences = gen_sents(hwt_trigram, 15)\n",
    "all_sentences = gen_sents(all_trigram, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep \n",
    "import random\n",
    "\n",
    "def print_sents(sentences):\n",
    "    for sent in sentences:\n",
    "        for c in sent:\n",
    "            sys.stdout.write(c)\n",
    "            sleep(random.uniform(0, 0.25))\n",
    "        print\n",
    "        sleep(random.uniform(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "June 25, 2008. \n",
      "Kathy, y'all take and y'all look after Aleda and make witnesses say what they have allowed me to be. \n",
      "Yes sir. \n",
      "I hold nothing against no one else was. \n",
      "If I could ask for. \n",
      "Be strong, brother. \n",
      "Get in church and get in church and get that for myself. \n",
      "All of you all, all of my famous legendary brother, Matt Turner, \"Y'all kiss my black ass.\" \n",
      "Please understand. \n",
      "To my loved ones and my family, grandson, friends. \n",
      "Amen. \n",
      "When I kill one or pop one, ya'll want to thank you for being here. \n"
     ]
    }
   ],
   "source": [
    "print_sents(ls_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when a bullet is the only part of the man. \n",
      "victims were found in different areas. \n",
      "recently. \n",
      "the man of course knew about. \n",
      "in sociology, in sociology. \n",
      "the man knew what he knew and had. \n",
      "there are many risk factors do not like being left­ handed. \n",
      "41 this man may never have dreamed. \n",
      "of the outdoor west entrance steps, placing them on their own sex, as well as firearms. \n",
      "it is illegal for him to decide to leave the library. \n",
      "the city was supposed to have launched over a very serious mental illness. \n"
     ]
    }
   ],
   "source": [
    "print_sents(hwt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the two­page letter was handwritten. \n",
      "You know that I am sorry that I am ready to begin with. \n",
      "the man thought to himself a good deal. \n",
      "this man was given to thinking of events of national importance. \n",
      "this includes taking away guns. \n",
      "many counties have laws that protect homosexuals from violence and discrimination. \n",
      "the man had many thoughts, few of them are over 200 metres tall, which is a list of friends. \n",
      "additionally, additionally, additionally, additionally, additionally, additionally, additionally. \n",
      "in the shooting, while six others received wounds requiring hospitalization. \n",
      "Lubbock County officials believe I could think of anything else. \n",
      "they may be voted upon by the courts. \n",
      "by crashing their planes, by crashing their planes, they would kill themselves as a nine. \n",
      "It's all good, it's been a nice person. \n",
      "I would like to tell you. \n",
      "Hector, you too. \n"
     ]
    }
   ],
   "source": [
    "print_sents(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "## Last Statements\n",
    "\n",
    "The Last Statements corpus is a collection of recorded last statements from prisoners on Death Row. The data is publicly available on the website of the Texas Department of Justice. While I originally had some ethical considerations when deciding whether or not to use this data, I convinced myself that my motivation for doing so was moral. I decided not to use a deep learning approach for text generation because I wanted the output to be intelligible and mirror the style of the text in the corpus. As a result, I decided to take an n-grams approach. The main motivation behind using this corpus is to analyze the general sentiment of these last statements and, perhaps, gain insight into the thoughts, concerns, and qualms going through these peoples' minds. \n",
    "\n",
    "## A Hard West Turn\n",
    "\n",
    "A Hard West Turn is a computationally generated book by Nick Montfort that is directly based on incidents of violence in recent American history. Using a computationally generated book as a corpus for computational text generation adds a layer of abstraction. I think it raises an interesting question regarding the ownership of the work. \n",
    "\n",
    "## Combining Last Statements and A Hard West Turn\n",
    "\n",
    "I'm somewhat hesistant to make assumptions about how these two corpora are related. However, my motivation for combining these texts was to juxtapose the emotions and thoughts of those affected by violence in America, from both sides of the equation. I was hoping to create a poem that displayed a mixture of emotions; I think the resulting poem succeeded in doing just that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
